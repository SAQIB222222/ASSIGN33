{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cdeeba8-08f5-4b15-9a6c-6703eb91ad95",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb2d3b-40c8-4077-bb58-fb6862019dda",
   "metadata": {},
   "source": [
    "Data extraction from websites using software or programmes is known as web scraping. Data is gathered using it for many different objectives, including research, analysis, machine learning, and more. Web scraping is a technique used to collect data from websites that publish data in a structured style, such as e-commerce sites, social media platforms, news websites, employment portals, and others. Web scraping is used to get data in three areas: \n",
    "\n",
    "E-commerce: Online scraping is used to gather information from e-commerce websites like Amazon, eBay, and Walmart about product costs, features, and customer reviews. \n",
    "\n",
    "Web scraping is a technique used to gather information from social networking sites like Twitter, Facebook, and LinkedIn about user profiles, postings, and comments.\n",
    "\n",
    "Research: Web scraping is used to gather information from websites like Google Scholar, PubMed, and IEEE Xplore about scientific papers, patents, and research reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5662b2-09be-413d-bc03-2dabcac7bc90",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a64bd7-4f6b-417a-ae03-070e821ecc25",
   "metadata": {},
   "source": [
    "Web scraping is done using a variety of techniques, some of which include: \n",
    "\n",
    "Web scraping manually entails manually copying and pasting data from webpages into a database or spreadsheet. \n",
    "It involves extracting data from HTML or XML files using regular expressions. \n",
    "HTML parsing: This process involves taking data from websites using HTML parsing libraries like Beautiful Soup and lxml. \n",
    "DOM parsing: It involves taking data from webpages using JavaScript libraries, such as jQuery. \n",
    "Web scraping tools: To automate the process of web scraping, use specialist web scraping programmes like Scrapy, Octoparse, and ParseHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eda9b5-5aeb-4335-beba-a6c2b41fcac9",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfda124-ec17-4ee1-9245-4ed5b431d062",
   "metadata": {},
   "source": [
    "A Python module called Lovely Soup is employed for web scraping. It is used to extract data from HTML and XML documents after they have been parsed. In addition to handling faulty HTML and XML markup, Beautiful Soup offers a straightforward user interface for navigating HTML and XML documents. In order to store the data in a structured format for further study, it is used to extract data from websites, including text, links, photos, and other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeff5f8-9853-46ed-9781-eb635021a5d0",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965293cc-bd89-4a52-a917-2d9cd43a59ed",
   "metadata": {},
   "source": [
    "A Python web framework called Flask is used to create web apps. It has a straightforward interface and is lightweight, making it easy to quickly construct web apps. The web scraping functionality in this project is exposed through a REST API built with Flask. The user inputs the URL of the website to be scraped into the Flask API, which then provides the scraped data in a JSON format. Because Flask makes building web applications easier and combines nicely with other Python frameworks like Beautiful Soup and requests, it is widely utilised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811f201-6476-44ef-92a7-b35a030ecd6a",
   "metadata": {},
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb907c-8d51-49f0-aea3-bd9854dd03c3",
   "metadata": {},
   "source": [
    "These AWS services are utilised in this web scraping project: \n",
    "\n",
    "The web scraping application is operated on virtual machines that are provisioned by EC2 (Elastic Compute Cloud). Scalable computing capacity is offered by EC2 in the cloud, and it is simple to modify to match the unique needs of the application. \n",
    "The scraped data is kept in S3 (Simple Storage Service). An S3 bucket, which offers highly scalable and reliable object storage in the cloud, is where the scraped data is kept. \n",
    "Lambda: The web scraping process is automated using lambda. Periodically, a Lambda function is called to start the web scraping procedure and save the collected data in an S3 bucket. In the cloud, Lambda offers serverless computing capability and can include"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
